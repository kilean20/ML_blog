{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards Principled Methods for Training Generative Adversarial Networks\n",
    "This note is a section by section summary of the [paper](https://arxiv.org/abs/1701.04862)  based on personal understanding. *This paper is under review for ICLR 2017.* Some personal remarks appear at the end of the note. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In general, the generative model often optimize maximumlikihood which is equivalent to the KL divergence minimization\n",
    "$$\n",
    "KL\\left( \\mathbb{p}_r | \\mathbb{p}_g \\right) = \\int_\\mathcal{X} p_r(x) \\log \\frac{p_r(x)}{p_g(x)} dx\n",
    "$$\n",
    "where $\\mathbb{p}_r$ and $\\mathbb{p}_g$ are random variables representing real and generator data, ${p}_r$, ${p}_g$ are corresponding densities.\n",
    "\n",
    "In GAN, the discriminator is trained to maximize [[Ref](2014_GAN.ipynb)]\n",
    "\n",
    "$$\n",
    "L(D,G) = \\mathbb{E}_{x\\sim \\mathbb{p}_r} \\left[\\log D(x)\\right] + \\mathbb{E}_{x\\sim \\mathbb{p}_g} \\left[\\log (1-D(x))\\right]\n",
    "$$\n",
    "\n",
    "where $p_r$ is the real data distribution, $p_g$ is the genrator distribution. Then the optimal discriminator has the shape \n",
    "\n",
    "$$\n",
    "D^*(x) = \\frac{p_r(x)}{p_r(x)+p_g(x)}\n",
    "$$\n",
    "\n",
    "When discriminator is optimum, \n",
    "\n",
    "$$\n",
    "L(D^*,G)=  2 JSD \\left( \\mathbb{p}_r | \\mathbb{p}_g \\right) -2 \\log 2\n",
    "$$\n",
    "\n",
    "where $JSD$ is the Jensen-Shanon divergence. It is conjectured that the reason of GANs success is due to the switch from the traditional maximum likelihood approaches to the $JSD$ [[Ref](https://arxiv.org/abs/1511.01844)]. \n",
    "\n",
    "**However, in practice** (experiementally) as the discriminator gets better, the updates to the generator get consistently worse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "## 2. Source of Instability\n",
    "\n",
    "According to the theory, the discriminator will have cost at most $2 \\log 2 - 2 JSD \\left( \\mathbb{p}_r | \\mathbb{p}_g \\right) $. \n",
    "However, in practice, if we just train D till convergence, its error will go to $0$, as observed in the following Figure (from the [paper](https://arxiv.org/abs/1701.04862)):\n",
    "\n",
    "<img src=\"GANtrainingTheoryFig1.png\" width=\"800\"/>\n",
    "\n",
    "It means the Jensen-Shannon divergence is maxed. **The only way this can happen is if the distributions are not continuous, or they have disjoint supports.**\n",
    "- One possible cause for the **discontinuos distribution** is if its **support lies on low dimensional manifold**\n",
    "    - Often $P_r$ is concentrated on a low dimensional manifold [Ref](https://papers.nips.cc/paper/2010/hash/8a1e808b55fde9455cb3d8857ed88389-Abstract.html)\n",
    "    - **Lemma 1.** The generator net $G(Z)$ is contained in a countable union of manifolds of dimension at most dim$(Z)$.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "### 2.1. The perfect discriminator theorems\n",
    "\n",
    "**Theorem 2.1.** If supports of $\\mathbb{p}_r$ and $\\mathbb{p}_g$ are in disjoint compact manifolds $\\mathcal{M}_r$ and $\\mathcal{M}_g$, then there exist perfect (i.e. accuracy is 1) discriminator for all $x\\in \\mathcal{M}_r \\cup \\mathcal{M}_g$\n",
    "\n",
    "**Theorem 2.2.** If supports of $\\mathbb{p}_r$ and $\\mathbb{p}_g$ are in closed compact manifolds $\\mathcal{M}_r$ and $\\mathcal{M}_g$ that do not align perfectly and do not have full diemsnion and $\\mathbb{p}_r$ and $\\mathbb{p}_g$ are continuous, then there exist perfect (i.e. accuracy is 1) discriminator for almost all $x\\in \\mathcal{M}_r \\cup \\mathcal{M}_g$\n",
    "\n",
    "**Theorem 2.3.** If supports of $\\mathbb{p}_r$ and $\\mathbb{p}_g$ are in manifolds $\\mathcal{M}_r$ and $\\mathcal{M}_g$ that do not align perfectly and do not have full diemsnion and $\\mathbb{p}_r$ and $\\mathbb{p}_g$ are continuous, then $JSD\\left( \\mathbb{p}_r | \\mathbb{p}_g \\right) = \\log2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "### 2.2. The consequences and the problems of each cost function\n",
    "consequences: Theorems 2.1 and 2.2 $\\rightarrow$ If supports of $\\mathbb{p}_r$ and $\\mathbb{p}_g$ are disjoint or lie on low dimensional manifolds, the optimal discriminator will be perfect having zero gradient.\n",
    "\n",
    "Next, we will look at gradient of generator...\n",
    "\n",
    "##### 2.2.1. original cost function\n",
    "\n",
    "**Corollary 2.1.**\n",
    "$$\n",
    "\\lim_{||D-D^*|| \\rightarrow 0} \\nabla_\\theta \\mathbb{E}_{z\\sim p(z)}\\left[ \\log \\left(1-D(G_\\theta(z))\\right)\\right] = 0\n",
    "$$\n",
    "\n",
    "This shows that as our discriminator gets better, the gradient of the generator vanishes.   \n",
    "(*It seems to be contratdicting Proposition 2. of the original GAN [paper](https://arxiv.org/abs/1406.2661). Maybe this is one of the reason why this paper is not accepted?*)\n",
    "\n",
    "##### 2.2.1. The $-\\log D$ alternative\n",
    "\n",
    "$$\n",
    "- \\nabla_\\theta \\mathbb{E}_{z\\sim p(z)}\\left[ \\log D^*(G_\\theta(z))\\right] = \\nabla_\\theta \\left[ KL\\left( p_g | p_r \\right) - 2 JSD \\left( p_g | p_r \\right) \\right]\n",
    "$$\n",
    "\n",
    "The negative Jensen-Shannon divergence push the two distribution away. *Note* also the order of arguemnts in KL divergence is oppoisite to the maximum likilihood. \n",
    "\n",
    "\n",
    "**Theorem 2.6** (Instability of generator gradient updates): The norm of gradient grows drastically as the discriminoator is trained close to optimality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "## 3. Toward softer metrics and distributions\n",
    "\n",
    "How to fix? Break the assumptions of previos theorems! \n",
    "\n",
    "- **by adding noise**:  $\\quad p_{x+\\epsilon}(x) \\equiv \\mathbb{E}_{y\\sim \\mathbb{p}_x} \\left[ p_\\epsilon(x-y) \\right]$\n",
    "\n",
    "\n",
    "**Corollary 3.2** Let $\\epsilon \\in \\mathcal{N}(0,\\sigma^2 I)$ are noises added to $p_r$ and $p_g$ and $\\tilde{G}_\\theta(z) \\equiv G_\\theta(z) + \\epsilon'$ then\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{z\\sim p(z), \\epsilon'} \\left[ \\nabla_\\theta \\log \\left( 1-D^*(\\tilde{G}_\\theta(z)) \\right) \\right] = 2 \\nabla_\\theta JSD \\left( \\mathbb{p}_{r+\\epsilon} || \\mathbb{p}_{g+\\epsilon} \\right)\n",
    "$$\n",
    "\n",
    "It means we are moving all our noisy samples towards the data manifold, which can be thought of as moving a small neighbourhood of samples towards it.\n",
    "\n",
    "- **a better alternative**:  Wasserstein metric\n",
    "$$\n",
    "W(\\mathbb{p},\\mathbb{q}) \\equiv \\inf_{\\gamma \\in \\Gamma} \\int_{\\chi \\times \\chi} ||x-y||_2 d\\gamma(x,y)\n",
    "$$\n",
    "where $\\Gamma$ is the set of all possible joints on $\\chi \\times \\chi$ that have marginals $\\mathbb{p}$ and $\\mathbb{q}$\n",
    "\n",
    "Note that the Wasserstein metric incorporates the notion of distance (as also seen inside the integral) between the elements in\n",
    "the support of $\\mathbb{p}$ and $\\mathbb{q}$. Furthermore, following Lemma shows convergence with added noise (which is not the case for $JSD$)\n",
    "\n",
    "**Lemma 4.** \n",
    "$$\n",
    "W(\\mathbb{p}_x,\\mathbb{p}_{x+\\epsilon}) \\le V^\\frac{1}{2}\n",
    "$$\n",
    "where $V=\\mathbb{E}\\left[ ||\\epsilon||^2_2\\right]$ is the variance.\n",
    "\n",
    "Finally,\n",
    "\n",
    "**Theorem 3.3.** If $\\epsilon$ is a random vector of 0 mean and variance $V$, and the supports of $\\mathbb{p}_{r+\\epsilon}$ and $\\mathbb{p}_{g+\\epsilon}$ are contained on a ball of diameter $C$, then\n",
    "$$\n",
    "W(\\mathbb{p}_r,\\mathbb{p}_{g}) \\le 2V^\\frac{1}{2} + 2C\\sqrt{JSD(\\mathbb{p}_{r+\\epsilon}||\\mathbb{p}_{g+\\epsilon})}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
