# [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)

## Introduction
The variational Bayesian (VB) approach involves the optimization (=maximizing variational lower bound) of an approximation to the intractable posterior.
A reparameterization trick yields a differentiable unbiased estimator of the lower bound.
Using neural network for the recognition model, we arrive at the variational auto-encoder allows us to perform very efficient approximate posterior inference.

## 

The [variational Bayesian](concept_Bayesian.ipynb)
 approach involves the optimization of variational model to the posterior which is often intractable.
- Stochastic Gradient Variational Bayes (SGVB) estimator (of the [variational lower bound](concept_Bayesian.ipynb) using a *reparameterization trick*) enable efficient approximate posterior inference.


- For i.i.d. datasets with continuous latent variables, the paper propose the Auto-Encoding VB (AEVB a.k.a **VAE**) algorithm.
   - for recognition, denoising, representation purposes.
