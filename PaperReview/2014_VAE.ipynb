{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Auto-Encoding Variational Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-[Generative model](generative_model.ipynb)\n",
    "\n",
    "-[Variational Inference](Bayesian.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note is a section by section summary of the [paper](https://arxiv.org/abs/1312.6114)  based on personal understanding. Some personal remarks at the end of the note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "$\\,$ The [variational Bayesian](concept_Bayesian.ipynb) (VB) approach involves the optimization (=maximizing variational *lower bound*) of an approximation to the intractable posterior.\n",
    "A *reparameterization trick* yields a differentiable unbiased estimator of the *lower bound* so that very efficient learning and approximate posterior inference (over latent variable) is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Method\n",
    "(assumption: i.i.d. dataset with latent variables per datapoint)\n",
    "\n",
    "\n",
    "#### 2.1. Problem scenario\n",
    "- prior over latent space $p_\\theta (z)$ where $\\theta$ is the model parameter\n",
    "- conditional over the domain of the data space $p_\\theta(x|z)$\n",
    "- problem: marginal likelihood $p_\\theta(x)$ and posterior $p_\\theta(z|x)$ are intractable\n",
    "\n",
    "\n",
    "#### 2.2. Variational bound\n",
    "$\\,$ Let $q_\\phi(z|x)$ be a recognition model (where $\\phi$ is the model parameter), i.e. variational approximation to the true posterior $p_\\theta(z|x)$. It can be interpreted as a probabilistic *encoder*. \n",
    "Then the variational [lower bound](concept_Bayesian.ipynb) can be written as (derivation is at the end of this notebook)\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\theta,\\phi;x) = \\mathbb{E}_{q_\\phi(z|x)}\\log p_\\theta(x|z)-D_{KL}\\left(q_\\phi(z|x)\\,||\\,p_\\theta(z)\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "where $D_{KL}$ is the KL-divergence. \n",
    "\n",
    "$\\,$ How can we optimize $\\mathcal{L}(\\theta,\\phi;x)$ efficiently?\n",
    "\n",
    "\n",
    "#### 2.3. SGVB estimator and AEVB algorithm\n",
    "- SGVB (Stochastic Gradient Variational Bayes)\n",
    "- AEVB (Auto-Encoding VB)\n",
    "\n",
    ">**reparameterization trick**\n",
    ": with an auxiliary random variable $\\epsilon$ and a differential transformation $g_\\phi(\\epsilon,x)$, a latent variable sample $z\\sim q_\\phi(z|x)$ can be reparameterized by\n",
    "$$z = g_\\phi(\\epsilon,x) \\quad \\mathrm{with} \\quad \\epsilon \\sim p(\\epsilon)$$\n",
    "<br>\n",
    "<font size=\"2\"> \n",
    "$\\qquad\\qquad$ (e.g.)  if $z \\sim q_\\phi(z|x)=\\mathcal{N}(\\mu(x),\\sigma^2(x))$, one can use  $z = g_\\phi(\\epsilon,x) =\\mu + \\sigma \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0,1)$\n",
    "</font>\n",
    "\n",
    "\n",
    "$\\,$ Using reparameterization trick SGVB estimator of lower bound is \n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\phi;x)  \\simeq \\frac{1}{L}\\sum_{l=1}^L \\left\\{\\, \\log p_\\theta (x,z^{(l)}) - \\log q_\\phi (z^{(l)}|x) \\,\\right\\}\n",
    "$$\n",
    "\n",
    "$\\,$ Often KL-divegence $D_{KL}\\left(q_\\phi(z|x)\\,||\\,p_\\theta(z)\\right)$ can be integrated analyitically. Then,\n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\phi;x)  \\simeq \\frac{1}{L}\\sum_{l=1}^L \\left\\{\\, \\log p_\\theta (x|z^{(l)}) - D_{KL}\\left(q_\\phi(z|x)\\,||\\,p_\\theta(z)\\right) \\,\\right\\}\n",
    "$$\n",
    "Note that $\\log p_\\theta (x|z^{(l)})$ can be interpreted as a negative *reconstruction error* in view of AEVB\n",
    "\n",
    "#### 2.4.  Reparametrization trick\n",
    "$\\,$ Recall that we are sample $z$ from $g_\\phi(\\epsilon,x)$ not from $q_\\phi(z|x)$. i.e., $g_\\phi(\\epsilon,x)$ reparameterize $q_\\phi(z|x)$ enabling SGVB. In order words, the Monte Carlo estimate of the expectation (over $q_\\phi(z|x)$) is differentiable w.r.t. $\\phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: Vaiational Auto-Encoder\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/3374/1*22cSCfmktNIwH5m__u2ffA.png\" width=\"600\"/>\n",
    "\n",
    "image source is [here](https://www.topbots.com/intuitively-understanding-variational-autoencoders/)\n",
    "\n",
    "- Let $p_\\theta(z) = \\mathcal{N}(z;0,I)$ \n",
    "<font size=\"1\"> (note that prior does not contain model paramemter in this case).</font> \n",
    "- Let also $p_\\theta(x|z)$ multivariate Gaussian\n",
    "\n",
    "Then, the true posterior is also multivariate Gaussian. If we further assume that the posterior covariance is diagonal,\n",
    "$$\n",
    "\\log q_\\phi (z|x) = log \\mathcal{N}(z;\\mu(x),\\sigma^2(x)I)\n",
    "$$\n",
    "\n",
    "Then,\n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\phi;x)  \\simeq \\frac{1}{L}\\sum_{l=1}^L \\log p_\\theta (x|z^{(l)}) + \\frac{1}{2}\\sum_{d=1}^D \\left( 1 + \\log \\sigma_d^2 - \\mu_d^2 - \\sigma_d^2\\right) \n",
    "$$\n",
    "\n",
    "where $D$ is the size of the latent space, and we used the followings\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int \\mathcal{N}(z;\\mu,\\sigma^2 I)\\log \\mathcal{N}(z;0,I) dz = &-\\frac{D}{2}\\log 2\\pi - \\frac{1}{2}\\sum_{d=1}^D \\left(\\mu_d^2 + \\sigma_d^2\\right) \\\\\n",
    "\\int \\mathcal{N}(z;\\mu,\\sigma^2 I)\\log \\mathcal{N}(z;\\mu,\\sigma^2 I) dz = &-\\frac{D}{2}\\log 2\\pi - \\frac{1}{2}\\sum_{d=1}^D \\left(1 + \\log \\sigma_d^2\\right) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Related work\n",
    "refer paper directly..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiments\n",
    "> \"Interestingly enough, more latent variables\n",
    "does not result in more overfitting, which is explained by the regularizing effect of the lower bound.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal remarks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation sketch of the lower bound\n",
    "\n",
    "\n",
    "We want the variational distribution $q_\\phi(z|x)$ to be as close as the true posterior $p_\\theta(z|x)$. The [KL divergene](concept_KLdiv.ipynb) of these two can be written as\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "D_{KL}\\left(q_{\\phi}(z|x)\\,||\\,p_{\\theta}(z|x)\\right)&=&\\mathbb{E}_{q_{\\phi}(z|x)}\\log\\frac{q_{\\phi}(z|x)}{p_{\\theta}(z|x)}\\\\&=&\\mathbb{E}_{q_{\\phi}(z|x)}\\log\\frac{q_{\\phi}(z|x)}{p_{\\theta}(z,x)/p_{\\theta}(x)}\\\\&=&\\mathbb{E}_{q_{\\phi}(z|x)}\\log\\frac{q_{\\phi}(z|x)}{p_{\\theta}(z,x)}+\\mathbb{E}_{q_{\\phi}(z|x)}\\log p_{\\theta}(x)\\\\&=&\\mathbb{E}_{q_{\\phi}(z|x)}\\log\\frac{q_{\\phi}(z|x)}{p_{\\theta}(z,x)}+\\log p_{\\theta}(x)\n",
    "\\end{eqnarray}\n",
    "$$ \n",
    "\n",
    "Because $D_{KL}\\ge 0$ by definition, the lower bound can be defined as\n",
    "$$\\mathcal{L}(\\theta,\\phi;x) = \\mathbb{E}_{q_\\phi(z|x)}\\log\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE for nonlinear ICA ( Indepedent Component Analysis )\n",
    "\n",
    "In the following assumption,\n",
    "$$\n",
    "\\log q_\\phi (z|x) = log \\mathcal{N}(z;\\mu(x),\\sigma^2(x)I)\n",
    "$$\n",
    "the latent variables are indepent to each other. Therefore, VAE can be used as a nonlinear ICA.\n",
    "\n",
    "In this [example]() I used VAE to find number of independent sources of time seires data\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
