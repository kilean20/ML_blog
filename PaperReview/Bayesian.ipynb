{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian inference\n",
    "\n",
    "A statistical inference method using Bayes' theorem to update hypothesis based on evidence (or observed data)\n",
    "\n",
    "Baye's theorem:\n",
    "$$p(z|x) = \\frac{p(x|z)p(z)}{p(x)}$$\n",
    "\n",
    "where $x$ is data or evidence, $p(x)$ is the model evidance or marginal likelihood, $ùëù(ùëß)$ is the prior or probability of the hypothesis, and $p(z|x)$ os the posterior, i.e. the probability of a hypothesis given the observed evidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Frequentist vs Bayesian picture\n",
    "\n",
    "- In a frequentist approach to inference, unknown parameters are often, but not always, treated as having fixed but unknown values\n",
    "\n",
    "- In contrast, a Bayesian approach to inference does allow probabilities to be associated with unknown parameters. This allows these probabilities to have an interpretation as representing the scientist's belief\n",
    "\n",
    "\n",
    "(e.g.) There exists two kinds of coins : loaded (70% head, 30% tail) and fair coin. You are asked to find the probability of a given coin to be loaded or fair coin by tossing it 4 times. After tossing it you got 2 heads. \n",
    "\n",
    "- Frequentist approach (using MLE), \n",
    "  - $L(\\mathrm{fair}|2) =  \\,_4C _2 \\times (0.5)^4 = 0.375$\n",
    "  - $L(\\mathrm{loaded}|2) =  \\,_4C _2 \\times (0.7)^2 \\times (0.3)^2 = 0.265$\n",
    "  - Therefore it is most likey that the coin is fair. Note that it is *point estimate*. It cannot answer how sure are you.\n",
    "\n",
    "\n",
    "- Bayesian approach. using $p(z|x) = p(x|z)p(z)/p(x)$\n",
    "   \n",
    "  - $p(\\mathrm{fair}|2) \\propto  0.375\\times p(\\mathrm{fair})$\n",
    "  - $p(\\mathrm{loaded}|2) \\propto  0.265\\times p(\\mathrm{loaded})$\n",
    "  - Here p(z) can be interpreted to be one's belief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Bayesian method\n",
    "\n",
    "$$\n",
    "p( z \\mid  x) = \\frac{p( x \\mid  z)p( z)}{p( x)} = \\frac{p( x \\mid  z)p( z)}{\\int_{ z} p( x, z) \\,d z}\n",
    "$$\n",
    "\n",
    "The marginalization over $z$ is typically intractable. \n",
    "Instead by introducing approximate posterior $q(z|x)\\simeq p(z|x)$, Bayesian inference becomes optimization (of evidence lower bound) problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evidence lower bound\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{x}) =\n",
    "D_{\\mathrm{KL}}\\left(q(\\mathbf{z}|\\mathbf{x}) \\parallel p(\\mathbf{z}|\\mathbf{x})\\right) - \\mathbb{E}_{q(\\mathbf{z}|\\mathbf{x}) } \\left[ \\log q(\\mathbf{z}|\\mathbf{x}) -  \\log p(\\mathbf{z},\\mathbf{x}) \\right] = D_{\\mathrm{KL}}\\left(q(\\mathbf{z}|\\mathbf{x}) \\parallel p(\\mathbf{z}|\\mathbf{x})\\right) + \\mathcal{L}(q)\n",
    "$$\n",
    "\n",
    "where $D_{KL} \\ge 0$ is the [KL divergene](concept_KLdiv.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
