{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "from copy import deepcopy as copy\n",
    "# np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/kilean20/pyTorchTemplate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/kilean20/pyTorchTemplate.git --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyTorchTemplate as ptt\n",
    "device = ptt.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4\n",
    "n = 2048  #number of samples\n",
    "nPrior = n*100\n",
    "p = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to fit\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "R &=& |\\boldsymbol{x}| \\\\\n",
    "{f} &=& \\frac{\\sin(4\\pi R)}{4\\pi R}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"The function to predict.\"\"\"\n",
    "    \n",
    "    R = np.sqrt(np.sum(x**2,axis=1)) + 0.0001\n",
    "    return np.sin(4*np.pi*R)/(4*np.pi*R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = ((np.random.rand(n,d)-0.5)*2).astype(np.float32)\n",
    "y_train = f(x_train)\n",
    "\n",
    "x_test = ((np.random.rand(n,d)-0.5)*2).astype(np.float32)\n",
    "y_test = f(x_test)\n",
    "\n",
    "x_onAxis = np.zeros([128,d],dtype=np.float32)\n",
    "x_onAxis[:,0] = np.linspace(-1, 1, 128)\n",
    "y_onAxis = f(x_onAxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train,bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without Prior model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = C(0.2, (2e-2, 1e1)) * RBF(1, (2e-2, 1e1))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.score(x_train, y_train), gp.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.fit(x_train, y_train)\n",
    "gp.kernel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.score(x_train, y_train), gp.score(x_test, y_test)  \n",
    "# score over test data after train can become even worse when train data is not enough !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, sigma = gp.predict(x_onAxis, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(x_onAxis[:,0],y_onAxis ,'k', label='Ground Truth')\n",
    "ax.plot(x_onAxis[:,0],y_pred,   'b-', label='Prediction')\n",
    "ax.fill_between(x_onAxis[:,0], y_pred- 1.96 * sigma,\n",
    "                               y_pred+ 1.96 * sigma, \n",
    "                label='95% confidence interval', color='C0', alpha=.5, )\n",
    "\n",
    "plt.title('1D slice view of '+str(d)+'D problem')\n",
    "plt.xlabel(r'$x_0$')\n",
    "plt.ylabel(r'$f(x)$')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-0.4,1.2)\n",
    "plt.xticks([-1,-0.5,0,0.5,1])\n",
    "plt.savefig('GP'+str(d)+'D_'+str(n)+'sample.png',dpi=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with Prior model\n",
    "Model true prior\n",
    "$$\n",
    "f_{0}(\\boldsymbol{x})=f(\\boldsymbol{x}) + a(\\boldsymbol{x})\\sin\\phi(\\boldsymbol{x}) + b(\\boldsymbol{x})\n",
    "$$\n",
    "\n",
    "where $a(\\boldsymbol{x})$, $\\phi(\\boldsymbol{x})$ and $b(\\boldsymbol{x})$ are modeled using randomized neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_func = torch.nn.Sequential(torch.nn.Linear(d,32),\n",
    "                               torch.nn.CELU(),\n",
    "                               torch.nn.Linear(32,32),\n",
    "                               torch.nn.CELU(),\n",
    "                               torch.nn.Linear(32,1))\n",
    "\n",
    "phase_func = torch.nn.Sequential(torch.nn.Linear(d,32),\n",
    "                                 torch.nn.CELU(),\n",
    "                                 torch.nn.Linear(32,32),\n",
    "                                 torch.nn.CELU(),\n",
    "                                 torch.nn.Linear(32,1))\n",
    "\n",
    "bias_func = torch.nn.Sequential(torch.nn.Linear(d,32),\n",
    "                                torch.nn.CELU(),\n",
    "                                torch.nn.Linear(32,32),\n",
    "                                torch.nn.CELU(),\n",
    "                                torch.nn.Linear(32,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()['0.weight'].copy_(model.state_dict()['0.weight']);\n",
    "# model.state_dict()['0.bias'].copy_(model.state_dict()['0.bias']);\n",
    "# model.state_dict()['2.weight'].copy_(model.state_dict()['2.weight']);\n",
    "# model.state_dict()['2.bias'].copy_(model.state_dict()['2.bias']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f0(x):\n",
    "    \"\"\"The approximate function to predict.\"\"\"\n",
    "    f0 = f(x)\n",
    "    amp = amp_func(torch.Tensor(2*x+2)).detach().numpy().ravel()\n",
    "    phase = phase_func(torch.Tensor(10*x+2)).detach().numpy().ravel()\n",
    "    bias = bias_func(torch.Tensor(2*x+2)).detach().numpy().ravel()\n",
    "    \n",
    "    return f0+0.5*(0.5*f0+amp)*np.sin(5*phase)-0.5*bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "plt.plot(x_onAxis[:,0],y_onAxis ,'k' , label='Ground Truth')\n",
    "plt.plot(x_onAxis[:,0],f0(x_onAxis),'r', label='Prior')\n",
    "plt.plot(x_onAxis[:,0],y_onAxis-f0(x_onAxis),'b:', label='Difference',alpha=0.5)\n",
    "\n",
    "\n",
    "plt.title('1D slice view of Prior')\n",
    "plt.xlabel(r'$x_0$')\n",
    "plt.ylabel(r'$f(x)$')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-0.4,1.2)\n",
    "plt.xticks([-1,-0.5,0,0.5,1])\n",
    "plt.savefig('Prior_'+str(d)+'D_SliceView.png',dpi=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using rough estimated data, we build the approximate prior close to the true prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xPrior_train = ((np.random.rand(nPrior,d)-0.5)*2).astype(np.float32)\n",
    "yPrior_train = f0(xPrior_train).reshape(-1,1).astype(np.float32)\n",
    "train_data_loader = torch.utils.data.DataLoader(list(zip(xPrior_train,yPrior_train)),batch_size=128*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(yPrior_train,bins=100);\n",
    "plt.xlabel('f')\n",
    "plt.ylabel('count')\n",
    "plt.title('train data histogram of prior')\n",
    "# plt.xlim(yPrior_train.min(),yPrior_train.max())\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('prior_train_data_histo_'+str(d)+'D_'+str(nPrior)+'sample.png',dpi=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,criterion,test_data_loader):\n",
    "  model.eval()\n",
    "  loss = 0 \n",
    "  for x, y in test_data_loader:\n",
    "    x = x.to(device)\n",
    "    y_pred = model(x)\n",
    "    loss += criterion(y_pred, y.to(device)).item()\n",
    "  return loss/len(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_best_loss = 1\n",
    "for i in range(10):\n",
    "  model = ptt.resFCNN([ndim,128,128,1], torch.nn.CELU(inplace=True))\n",
    "  mode,hist = ptt.train_supervised(model,1.0e-2,10,\n",
    "                                  train_data_loader,\n",
    "                                  optimizer=torch.optim.Adam,\n",
    "                                  criterion=ptt.MPELoss(p=p),\n",
    "                                  old_best_loss = old_best_loss,\n",
    "                                  dispHead = 0, dispTail = 0)\n",
    "  newloss = test(model,ptt.MPELoss(p=p),train_data_loader)\n",
    "  if newloss > 1e-4:\n",
    "    continue\n",
    "  mode,hist = ptt.train_supervised(model,3.3e-3,20,\n",
    "                                  train_data_loader,\n",
    "                                  optimizer=torch.optim.Adam,\n",
    "                                  criterion=ptt.MPELoss(p=p),\n",
    "                                  old_hist = hist,\n",
    "                                  old_best_loss = old_best_loss,\n",
    "                                  dispHead = 0, dispTail = 0)\n",
    "  newloss = test(model,ptt.MPELoss(p=p),train_data_loader)\n",
    "  if newloss > 1e-5:\n",
    "    continue\n",
    "  mode,hist = ptt.train_supervised(model,1.0e-3,30,\n",
    "                                  train_data_loader,\n",
    "                                  optimizer=torch.optim.Adam,\n",
    "                                  criterion=ptt.MPELoss(p=p),\n",
    "                                  old_hist = hist,\n",
    "                                  old_best_loss = old_best_loss,\n",
    "                                  dispHead = 0, dispTail = 0)\n",
    "  newloss = test(model,ptt.MPELoss(p=p),train_data_loader)\n",
    "  \n",
    "  if newloss < old_best_loss:\n",
    "    old_best_loss = newloss\n",
    "    final_model = copy(model)\n",
    "    final_hist  = copy(hist)\n",
    "    \n",
    "  if newloss < 1e-6:\n",
    "    break\n",
    "    \n",
    "  plt.figure(figsize=(4,2))\n",
    "  plt.semilogy(hist['train_loss'])\n",
    "  plt.semilogy(hist['test_loss'])\n",
    "\n",
    "model = final_model\n",
    "hist = final_hist\n",
    "mode,hist = ptt.train_supervised(model,1.0e-4,40,\n",
    "                                train_data_loader,\n",
    "                                optimizer=torch.optim.Adam,\n",
    "                                criterion=ptt.MPELoss(p=p),\n",
    "                                old_hist = hist,\n",
    "                                old_best_loss = newloss,\n",
    "                                dispHead = 0, dispTail = 0)\n",
    "newloss = test(model,ptt.MPELoss(p=p),train_data_loader)\n",
    "\n",
    "\n",
    "model = model.cpu()\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(hist['train_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return model(torch.Tensor(x)).detach().numpy().flatten()\n",
    "\n",
    "y_train = f(x_train) -f1(x_train)\n",
    "\n",
    "y_test = f(x_test) -f1(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "plt.plot(x_onAxis[:,0],y_onAxis ,'k' , label='Ground Truth')\n",
    "plt.plot(x_onAxis[:,0],f0(x_onAxis),'r:', label='Prior',alpha=0.7)\n",
    "plt.plot(x_onAxis[:,0],f1(x_onAxis),'r', label='Data Driven Prior')\n",
    "plt.plot(x_onAxis[:,0],y_onAxis-f1(x_onAxis),'b:', label='Difference',alpha=0.5)\n",
    "\n",
    "\n",
    "plt.title('1D slice view of Prior')\n",
    "plt.xlabel(r'$x_0$')\n",
    "plt.ylabel(r'$f(x)$')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-0.4,1.2)\n",
    "plt.xticks([-1,-0.5,0,0.5,1])\n",
    "plt.savefig('DataDrivenPrior_'+str(d)+'D_SliceView.png',dpi=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = C(0.2, (2e-2, 1e1)) * RBF(1, (2e-2, 1e1))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.score(x_train, y_train), gp.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.fit(x_train, y_train)\n",
    "gp.kernel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.score(x_train, y_train), gp.score(x_test, y_test)  \n",
    "# score over test data after train can become even worse when train data is not enough !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, sigma = gp.predict(x_onAxis, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(x_onAxis[:,0],y_onAxis           ,'k' , label='Ground Truth')\n",
    "ax.plot(x_onAxis[:,0],f1(x_onAxis)       ,'r',  label='Data Driven Prior')\n",
    "ax.plot(x_onAxis[:,0],f1(x_onAxis)+y_pred,'b-', label='Prediction')\n",
    "ax.fill_between(x_onAxis[:,0], \n",
    "                f1(x_onAxis)+y_pred- 1.96 * sigma,\n",
    "                f1(x_onAxis)+y_pred+ 1.96 * sigma, \n",
    "                label='95% confidence interval', color='C0', alpha=.5, )\n",
    "\n",
    "plt.title('1D slice view of '+str(d)+'D problem with data driven prior')\n",
    "plt.xlabel(r'$x_0$')\n",
    "plt.ylabel(r'$f(x)$')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-0.4,1.2)\n",
    "plt.xticks([-1,-0.5,0,0.5,1])\n",
    "plt.savefig('GP_wDataDrivenPrior_'+str(d)+'D_'+str(n)+'sample_'+str(nPrior)+'priorSample.png',dpi=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-v1.4.0",
   "language": "python",
   "name": "pytorch-v1.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
